{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsZ72WHq36Qw"
      },
      "source": [
        "### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5SLrzrQ5m8ej"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad6VuSShjzaK"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uXNuUAuCmI29"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIVXOiXY4KOf"
      },
      "source": [
        "## **Downloading Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xzklUaso4K9-"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE7O0-uYV77y"
      },
      "outputs": [],
      "source": [
        "fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h5aD7MC4dX9"
      },
      "source": [
        "## **Data pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl1WSxUAXe0g"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing\n",
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "\n",
        "# Print the dimensions of the dataset\n",
        "print('Train: X = ', trainX.shape)\n",
        "print('Test: X = ', testX.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x_mMsZJTm3Ms"
      },
      "outputs": [],
      "source": [
        "class_labels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTwYJNrQbdfB"
      },
      "source": [
        "### **Q1**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnC-fOf0SPWs"
      },
      "outputs": [],
      "source": [
        "j=0 #j corresponds to the spot index in the subplot \n",
        "classSet=set()\n",
        "Images=[]\n",
        "for i in range(100,150):#i corresponds to the img index \n",
        "  if(j>9):\n",
        "    break\n",
        "  if(trainY[i] not in classSet):\n",
        "    classSet.add(trainY[i])\n",
        "    Images.append(wandb.Image(trainX[i], caption=class_labels[trainY[i]]))\n",
        "    plt.subplot(2,5,j+1);j+=1\n",
        "    plt.imshow(trainX[i], cmap=\"Greys\")\n",
        "    plt.axis('off') # off the axis\n",
        "    plt.title('{}'.format(class_labels[trainY[i]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ocs0rHUm6Gg"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"project1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5OJZw8WEkCY5"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"Examples for each class\": Images})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNp98g3RoGEP"
      },
      "source": [
        "### **Q2**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9_rxyK3bph"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "06p8_mPxt0Io"
      },
      "outputs": [],
      "source": [
        "trainX = trainX.reshape(trainX.shape[0], 784)\n",
        "testX = testX.reshape(testX.shape[0], 784)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfDdSrxou5zh",
        "outputId": "e98f6bb0-43e9-46e2-b2e5-0cbba85f6a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "#feature Scaling\n",
        "trainX=trainX/255.0\n",
        "testX=testX/255.0\n",
        "\n",
        "print(trainX.shape)\n",
        "print(testX.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kQUZxSkXvs-3"
      },
      "outputs": [],
      "source": [
        "# Split the X_train into a training set and validation set\n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1, random_state=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHf5k9A1dR8m",
        "outputId": "60822e70-a7ae-4d28-b96b-6894cb0a8251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784)\n",
            "(10000, 784)\n",
            "(6000, 784)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(trainX.shape)\n",
        "print(testX.shape)\n",
        "print(valX.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzN3d-4jdyg8",
        "outputId": "32b7a4ab-9d50-4ee7-c3b2-37d4de42713a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000,)\n",
            "(10000,)\n",
            "(6000,)\n"
          ]
        }
      ],
      "source": [
        "print(trainY.shape)\n",
        "print(testY.shape)\n",
        "print(valY.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwGOAFWEljs5"
      },
      "source": [
        "## **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "srGUnixcawq5"
      },
      "outputs": [],
      "source": [
        "def sigmoid(a):\n",
        "    return 1.0 / (np.exp(-a)+1.0)\n",
        "\n",
        "def softmax(a):\n",
        "    return np.exp(a)/np.sum(np.exp(a),axis=0)\n",
        "\n",
        "def grad_sigmoid(a):\n",
        "  return g(a)*(1-g(a))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def grad_relu(x):\n",
        "    return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "\n",
        "def grad_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSibF6j1kVoi"
      },
      "source": [
        "### **Q3**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAtL-uWE3u6L",
        "outputId": "de808cd3-63fe-48df-9f3a-85cdf120ec13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "numSamples=trainX.shape[0]\n",
        "numSamples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhppOE_5_GSK"
      },
      "source": [
        "### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rELPAHy-N1A3"
      },
      "outputs": [],
      "source": [
        "class model():\n",
        "  def __init__(self,numLayers,numNeurons,optimizer,activation_funtion,initialization): \n",
        "    self.numLayers=numLayers #3 hidden layers\n",
        "    self.numHiddenLayers=self.numLayers-1\n",
        "    self.numNeurons=numNeurons\n",
        "    self.numClasses=10\n",
        "    self.grad_w=[]\n",
        "    self.grad_b=[]\n",
        "    self.y_pred=[]\n",
        "    self.u_w=0\n",
        "    self.u_b=0\n",
        "    self.m_w=0\n",
        "    self.m_b=0\n",
        "    self.W_L=[]\n",
        "    self.b_L=[]\n",
        "    self.optimizer=optimizer\n",
        "    if(activation_funtion==\"sigmoid\"):\n",
        "      self.g=sigmoid\n",
        "      self.grad_activation=grad_sigmoid\n",
        "    if(activation_funtion==\"relu\"):\n",
        "      self.g=relu\n",
        "      self.grad_activation=grad_relu\n",
        "    if(activation_funtion==\"tanh\"):\n",
        "      self.g=tanh\n",
        "      self.grad_activation=grad_tanh\n",
        "    self.initialization=initialization\n",
        "      \n",
        "\n",
        "  def initialize(self):\n",
        "    #Initialising weights and Biases\n",
        "    if self.initialization==\"random_uniform\":\n",
        "      W = []\n",
        "      W.append((np.random.uniform(-1,1,(784,self.numNeurons))))\n",
        "      for i in range (2 , self.numHiddenLayers+1): #Hiddenlayer 1 to last hidden layer (starts from 2 coz first layer is init just above)\n",
        "        W.append((np.random.uniform(-1,1,(self.numNeurons,self.numNeurons))))\n",
        "      W.append((np.random.uniform(-1,1,(self.numNeurons,self.numClasses))))\n",
        "      self.W= np.array(W)\n",
        "\n",
        "    if self.initialization==\"xavier\":\n",
        "      W = []\n",
        "      W.append((np.random.uniform(-1,1,(784,self.numNeurons)))*np.sqrt(2/(784+self.numNeurons)))\n",
        "      for i in range (2 , self.numHiddenLayers+1): #Hiddenlayer 1 to last hidden layer (starts from 2 coz first layer is init just above)\n",
        "        W.append((np.random.uniform(-1,1,(self.numNeurons,self.numNeurons)))*np.sqrt(2/(self.numNeurons+self.numNeurons)))\n",
        "      W.append((np.random.uniform(-1,1,(self.numNeurons,self.numClasses)))*np.sqrt(2/(self.numNeurons+self.numClasses)))\n",
        "      self.W= np.array(W)\n",
        "\n",
        "    b = []\n",
        "    for i in range (1 , self.numLayers): #Hiddenlayer1 to last hidden layer\n",
        "      b.append(np.zeros((self.numNeurons,1)))\n",
        "    b.append(np.zeros((self.numClasses,1)))\n",
        "    self.b= np.array(b)\n",
        "\n",
        "  def back_propagation(self,Y,batch_size):\n",
        "    grad_a=[None]*(self.numLayers)\n",
        "    grad_b=[None]*(self.numLayers)\n",
        "    grad_h=[None]*(self.numLayers)\n",
        "    grad_w=[None]*(self.numLayers)\n",
        "    oneHot_y=self.compute_oneHot_y(Y)\n",
        "\n",
        "    grad_a[self.numLayers-1]=self.y_pred-oneHot_y.T #k-dim\n",
        "    h=self.activation\n",
        "    a=self.preActivation\n",
        "    W=self.W\n",
        "    for k in range (self.numLayers-1,-1,-1): #reverse loop\n",
        "\n",
        "      grad_w[k]=np.matmul(grad_a[k], h[k].T)\n",
        "      grad_b[k]=np.sum(grad_a[k],axis=1,keepdims=True)/batch_size\n",
        "      grad_h[k]=np.matmul(W[k],grad_a[k])\n",
        "\n",
        "      if(k>0):\n",
        "        grad_a[k-1] =grad_h[k] * self.grad_activation(a[k-1])\n",
        "    self.grad_b,self.grad_w=grad_b,grad_w\n",
        "\n",
        "  def feed_forward(self,X):\n",
        "    a=[None]*(self.numLayers)\n",
        "    h=[None]*(self.numLayers)\n",
        "    k=0\n",
        "    h[0]=X.T\n",
        "    for k in range(0, self.numLayers-1): #for all layers\n",
        "      a_k=self.b[k]+np.matmul(self.W[k].T,h[k]) #0-based Indexing\n",
        "      h_k=self.g(a_k)\n",
        "      a[k]=(a_k)\n",
        "      h[k+1]=(h_k)\n",
        "    a[self.numLayers-1]=self.b[self.numLayers-1]+np.matmul(self.W[self.numLayers-1].T,h[self.numLayers-1])\n",
        "\n",
        "   \n",
        "    output=softmax(a[self.numLayers-1])\n",
        "    \n",
        "\n",
        "    self.activation,self.preActivation=h,a\n",
        "    self.y_pred=output\n",
        "\n",
        "  def train(self,trainX,trainY,batch_size,epochs,beta,beta_1,beta_2,neta,t):\n",
        "    self.initialize()\n",
        "\n",
        "    if self.optimizer=='nesterov':\n",
        "      self.init_nag()\n",
        "\n",
        "    for j in range(0,epochs):\n",
        "      for i in range(0, trainX.shape[0],batch_size):\n",
        "       \n",
        "        if (self.optimizer == 'nesterov'):\n",
        "          W_copy=self.W\n",
        "          b_copy=self.b\n",
        "          self.pre_update_nag(neta)\n",
        "\n",
        "\n",
        "        self.feed_forward(trainX[i:i+batch_size])\n",
        "        self.back_propagation(trainY[i:i+batch_size],batch_size)\n",
        "        Grad_w=np.array(self.grad_w)\n",
        "        for i in range(0,Grad_w.shape[0]):\n",
        "          Grad_w[i]=Grad_w[i].T\n",
        "        Grad_b=np.array(self.grad_b)\n",
        "\n",
        "        if (self.optimizer == 'sgd'):\n",
        "          self.update_sgd(neta,Grad_w,Grad_b)\n",
        "        if (self.optimizer == 'momentum'): \n",
        "          self.update_mom(neta,beta,Grad_w,Grad_b)\n",
        "        if (self.optimizer == 'nesterov'):\n",
        "          self.update_nag(neta,beta,Grad_w,Grad_b,W_copy,b_copy)\n",
        "        if (self.optimizer == 'rmsProp'):\n",
        "          self.update_rmsProp(neta,beta,Grad_w,Grad_b)\n",
        "        if (self.optimizer == 'adam'):\n",
        "          self.update_adam(neta,beta_1,beta_2,Grad_w,Grad_b,t)\n",
        "        if (self.optimizer == 'nadam'):\n",
        "          self.update_nadam(neta,beta_1,beta_2,Grad_w,Grad_b,t)\n",
        "\n",
        "        #self.update_new_optimiser()     The new optimiser algorithm will be called here\n",
        "        t+=1\n",
        "\n",
        "      self.feed_forward(trainX)\n",
        "      loss=self.compute_loss(self.y_pred,self.compute_oneHot_y(trainY),trainX.shape[0])\n",
        "      print(loss)\n",
        "      #loss is computed over the entire Xtrain (excluding valX).\n",
        "      wandb.log({\"loss \": loss})\n",
        "\n",
        "  def test(self,testX,testY,beta,neta):\n",
        "    self.feed_forward(testX)\n",
        "    print(\"Accuracy: \")\n",
        "    val_acc=self.get_accuracy(testY,self.y_pred.T)*100\n",
        "    print(val_acc)\n",
        "    wandb.log({\"validation_accuracy \": val_acc})\n",
        "\n",
        "\n",
        "  def update_sgd(self,neta,Grad_w,Grad_b):\n",
        "        self.W=self.W-neta*Grad_w\n",
        "        self.b=self.b-neta*Grad_b\n",
        "  \n",
        "  def update_mom(self,neta,beta,Grad_w,Grad_b):\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b\n",
        "        self.W=self.W-neta*self.u_w\n",
        "        self.b=self.b-neta*self.u_b\n",
        "  \n",
        "  def update_nag(self,neta,beta,Grad_w,Grad_b,W_copy,b_copy):\n",
        "        self.W=W_copy\n",
        "        self.b=b_copy\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b\n",
        "        self.W=self.W-neta*self.u_w\n",
        "        self.b=self.b-neta*self.u_b\n",
        "\n",
        "\n",
        "  def init_nag(self):\n",
        "    W_L = []\n",
        "    W_L.append((np.zeros([784,self.numNeurons])))\n",
        "    for i in range (2 , self.numHiddenLayers+1): #Hiddenlayer 1 to last hidden layer (starts from 2 coz first layer is init just above)\n",
        "      W_L.append((np.zeros([self.numNeurons,self.numNeurons])))\n",
        "    W_L.append((np.zeros([self.numNeurons,self.numClasses])))\n",
        "    W_L= np.array(W_L)\n",
        "\n",
        "    b_L = []\n",
        "    for i in range (1 , self.numLayers): #Hiddenlayer1 to last hidden layer\n",
        "      b_L.append(np.zeros((self.numNeurons,1)))\n",
        "    b_L.append(np.zeros((self.numClasses,1)))\n",
        "    b_L= np.array(b_L)\n",
        "    self.W_L=W_L\n",
        "    self.b_L=b_L\n",
        "\n",
        "  def pre_update_nag(self,neta):\n",
        "    self.W_L=self.W-neta*self.u_w\n",
        "    self.b_L=self.b-neta*self.u_b\n",
        "    self.W=self.W_L\n",
        "    self.b=self.b_L\n",
        "\n",
        "  def update_rmsProp(self,neta,beta,Grad_w,Grad_b):\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b*Grad_b\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*Grad_w\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*Grad_b\n",
        "  \n",
        "  def update_adam(self,neta,beta_1,beta_2,Grad_w,Grad_b,t):\n",
        "\n",
        "        self.m_w=beta_1*self.m_w+(1-beta_1)*Grad_w\n",
        "        self.m_b=beta_1*self.m_b+(1-beta_1)*Grad_b\n",
        "\n",
        "        self.u_w=beta_2*self.u_w+(1-beta_2)*Grad_w*Grad_w\n",
        "        self.u_b=beta_2*self.u_b+(1-beta_2)*Grad_b*Grad_b\n",
        "\n",
        "        m_w_hat=self.m_w/(1-beta_1**t)\n",
        "        m_b_hat=self.m_b/(1-beta_1**t)\n",
        "\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*m_w_hat\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*m_b_hat\n",
        "\n",
        "  def update_nadam(self,neta,beta_1,beta_2,Grad_w,Grad_b,t):\n",
        "\n",
        "        self.m_w=beta_1*self.m_w+(1-beta_1)*Grad_w\n",
        "        self.m_b=beta_1*self.m_b+(1-beta_1)*Grad_b\n",
        "\n",
        "        self.u_w=beta_2*self.u_w+(1-beta_2)*Grad_w*Grad_w\n",
        "        self.u_b=beta_2*self.u_b+(1-beta_2)*Grad_b*Grad_b\n",
        "\n",
        "        m_w_hat=self.m_w/(1-beta_1**t)\n",
        "        m_b_hat=self.m_b/(1-beta_1**t)\n",
        "\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*(beta_1*m_w_hat+((1-beta_1)*Grad_w)/(1-beta_1**t))\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*(beta_1*m_b_hat+((1-beta_1)*Grad_b)/(1-beta_1**t))\n",
        "\n",
        "  \"\"\"\"def update_new_optimiser(): \n",
        "        Update function for a new optimisation algorithm to be entered here \"\"\"\n",
        "\n",
        "  def compute_oneHot_y(self,Y):\n",
        "    oneHot_y=[]\n",
        "    for i in range(0,Y.shape[0]):\n",
        "      temp=np.zeros(self.numClasses)\n",
        "      temp[Y[i]]=1\n",
        "      oneHot_y.append(temp)\n",
        "    oneHot_y=np.array(oneHot_y)\n",
        "    return oneHot_y\n",
        "\n",
        "  def compute_loss(self,y_pred,oneHot_y,numImages):\n",
        "     return (-1.0 * np.sum(np.multiply(oneHot_y.T, np.log(y_pred)))/numImages)\n",
        "\n",
        "\n",
        "  def get_accuracy(self,Y_true,Y_pred):\n",
        "    size=Y_true.shape[0]\n",
        "    corrects = 0\n",
        "    s=0\n",
        "    while(s<size):\n",
        "      t=0\n",
        "      maxT = 0\n",
        "      maxS = 0\n",
        "\n",
        "      while (t<10):\n",
        "        if (maxS < Y_pred[s][t]):\n",
        "          maxT = t\n",
        "          maxS = Y_pred[s][t]\n",
        "        t+=1\n",
        "      if (maxT == Y_true[s]):\n",
        "        corrects+=1\n",
        "      accuracy=corrects/size\n",
        "      s+=1\n",
        "    return accuracy\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment 1\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"random\",\n",
        "  \"parameters\": {\n",
        "       \"activation_funtion\": {\n",
        "            \"values\": [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"values\": [\"xavier\", \"random_uniform\"]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.01,0.001, 0.0001]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nesterov\", \"adam\", \"nadam\", \"RMSprop\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32,64,128,256,512]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10] #[5,10,20]\n",
        "        },\n",
        "        # \"L2_lamb\": {\n",
        "        #     \"values\": [0, 0.0005, 0.5]\n",
        "        # },\n",
        "         \"numLayers\": {\n",
        "            \"values\": [4, 5, 6]\n",
        "        },\n",
        "\n",
        "        \"numNeurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "_DIBPdppn7Hn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sweep():\n",
        "  wandb.init()\n",
        "  config = wandb.config\n",
        "  obj = model(config.numLayers,config.numNeurons,config.optimizer,config.activation_funtion,config.initialization)\n",
        "\n",
        "  beta=0.999\n",
        "  beta_1=0.9\n",
        "  beta_2=0.999\n",
        "\n",
        "  t = 1 # initialize timestep for Adam optimizer\n",
        "\n",
        "  obj.train(trainX,trainY,config.batch_size,config.epochs,beta,beta_1,beta_2,config.learning_rate,t)\n",
        "\n",
        "  #there is no back_prop in test.. so need not actually give beta, beta_1, neta, t etc..!\n",
        "  obj.test(testX,testY,beta,config.learning_rate)\n",
        "\n",
        "   # Meaningful name for the run\n",
        "  wandb.run.name = \"run_Name\"\n",
        "  wandb.run.save()"
      ],
      "metadata": {
        "id": "3T32Zg0LVu7l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config,project=\"project1\")\n",
        "wandb.agent(sweep_id, sweep, count=5)"
      ],
      "metadata": {
        "id": "TJvkGJdiRlNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u85MBMDbBNAk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vNp98g3RoGEP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}