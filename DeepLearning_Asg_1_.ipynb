{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IsZ72WHq36Qw",
        "lIVXOiXY4KOf",
        "5h5aD7MC4dX9",
        "TTwYJNrQbdfB",
        "vNp98g3RoGEP",
        "RSibF6j1kVoi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "IsZ72WHq36Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "5SLrzrQ5m8ej"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb"
      ],
      "metadata": {
        "id": "Ad6VuSShjzaK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb"
      ],
      "metadata": {
        "id": "uXNuUAuCmI29"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Downloading Data**"
      ],
      "metadata": {
        "id": "lIVXOiXY4KOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "xzklUaso4K9-"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE7O0-uYV77y",
        "outputId": "b34ba276-4490-45ec-8788-ef4c677dcd28"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'keras.datasets.fashion_mnist' from '/usr/local/lib/python3.8/dist-packages/keras/datasets/fashion_mnist.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "id": "Y3z-vibRWGu3"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data pre-processing**"
      ],
      "metadata": {
        "id": "5h5aD7MC4dX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing\n",
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "\n",
        "# Print the dimensions of the dataset\n",
        "print('Train: X = ', trainX.shape)\n",
        "print('Test: X = ', testX.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl1WSxUAXe0g",
        "outputId": "ba552a4c-6c54-4dad-a446-c60f6fed4234"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X =  (60000, 28, 28)\n",
            "Test: X =  (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
      ],
      "metadata": {
        "id": "x_mMsZJTm3Ms"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TTwYJNrQbdfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# j=0 #j corresponds to the spot index in the subplot \n",
        "# classSet=set()\n",
        "# Images=[]\n",
        "# for i in range(100,150):#i corresponds to the img index \n",
        "#   if(j>9):\n",
        "#     break\n",
        "#   if(trainY[i] not in classSet):\n",
        "#     classSet.add(trainY[i])\n",
        "#     Images.append(wandb.Image(trainX[i], caption=class_labels[trainY[i]]))\n",
        "#     plt.subplot(2,5,j+1);j+=1\n",
        "#     plt.imshow(trainX[i], cmap=\"Greys\")\n",
        "#     plt.axis('off') # off the axis\n",
        "#     plt.title('{}'.format(class_labels[trainY[i]]))"
      ],
      "metadata": {
        "id": "SnC-fOf0SPWs"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init()"
      ],
      "metadata": {
        "id": "8Ocs0rHUm6Gg"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.log({\"Examples for each class\": Images})\n"
      ],
      "metadata": {
        "id": "5OJZw8WEkCY5"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vNp98g3RoGEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "lL9_rxyK3bph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = trainX.reshape(trainX.shape[0], 784)\n",
        "testX = testX.reshape(testX.shape[0], 784)"
      ],
      "metadata": {
        "id": "06p8_mPxt0Io"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature Scaling\n",
        "trainX=trainX/255.0\n",
        "testX=testY/255.0"
      ],
      "metadata": {
        "id": "xfDdSrxou5zh"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the X_train into a training set and validation set\n",
        "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.2, random_state=100)"
      ],
      "metadata": {
        "id": "kQUZxSkXvs-3"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Functions**"
      ],
      "metadata": {
        "id": "pwGOAFWEljs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g(a):        #sigmoid\n",
        "    # a=a-np.max(a)\n",
        "\n",
        "    # a = np.float128(a)\n",
        "    return 1.0 / (np.exp(-a)+1.0)\n",
        "\n",
        "def o(a):#softmax\n",
        "    # a=a-np.max(a) \n",
        "    # a = np.float128(a)\n",
        "    return np.exp(a)/np.sum(np.exp(a),axis=0)\n",
        "\n",
        "def grad_sigmoid(a):\n",
        "  return g(a)*(1-g(a))\n",
        "\n",
        "def Relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def grad_Relu(x):\n",
        "    return 1*(x>0) \n",
        "\n",
        "def grad_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n"
      ],
      "metadata": {
        "id": "srGUnixcawq5"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3**\n",
        "\n"
      ],
      "metadata": {
        "id": "RSibF6j1kVoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numSamples=trainX.shape[0]\n",
        "numSamples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAtL-uWE3u6L",
        "outputId": "c2f660ea-3d47-4115-a798-69655acddc4f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48000"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "zhppOE_5_GSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model():\n",
        "  def __init__(self): \n",
        "    self.numLayers=4 #3 hidden layers\n",
        "    self.numHiddenLayers=self.numLayers-1\n",
        "    self.numNeurons=32\n",
        "    self.numClasses=10\n",
        "    self.grad_w=[]\n",
        "    self.grad_b=[]\n",
        "    self.y_pred=[]\n",
        "    self.u_w=0\n",
        "    self.u_b=0\n",
        "    self.m_w=0\n",
        "    self.m_b=0\n",
        "    self.W_L=[]\n",
        "    self.b_L=[]\n",
        "\n",
        "\n",
        "  def initialize(self):\n",
        "    #Initialising weights and Biases\n",
        "    W = []\n",
        "    W.append((np.random.uniform(-1,1,(784,self.numNeurons))))\n",
        "    for i in range (2 , self.numHiddenLayers+1): #Hiddenlayer 1 to last hidden layer (starts from 2 coz first layer is init just above)\n",
        "      W.append((np.random.uniform(-1,1,(self.numNeurons,self.numNeurons))))\n",
        "    W.append((np.random.uniform(-1,1,(self.numNeurons,self.numClasses))))\n",
        "    self.W= np.array(W)\n",
        "\n",
        "    b = []\n",
        "    for i in range (1 , self.numLayers): #Hiddenlayer1 to last hidden layer\n",
        "      b.append(np.zeros((self.numNeurons,1)))\n",
        "    b.append(np.zeros((self.numClasses,1)))\n",
        "    self.b= np.array(b)\n",
        "\n",
        "  def back_propagation(self,Y,batch_size):\n",
        "    grad_a=[None]*(self.numLayers)\n",
        "    grad_b=[None]*(self.numLayers)\n",
        "    grad_h=[None]*(self.numLayers)\n",
        "    grad_w=[None]*(self.numLayers)\n",
        "    oneHot_y=self.compute_oneHot_y(Y)\n",
        "\n",
        "    grad_a[self.numLayers-1]=self.y_pred-oneHot_y.T #k-dim\n",
        "    h=self.activation\n",
        "    a=self.preActivation\n",
        "    W=self.W\n",
        "    for k in range (self.numLayers-1,-1,-1): #reverse loop\n",
        "\n",
        "      grad_w[k]=np.matmul(grad_a[k], h[k].T)\n",
        "      grad_b[k]=np.sum(grad_a[k],axis=1,keepdims=True)/batch_size\n",
        "      grad_h[k]=np.matmul(W[k],grad_a[k])\n",
        "\n",
        "      # print(grad_a[k].shape, h[k].shape,grad_w[k].shape)\n",
        "\n",
        "      if(k>0):\n",
        "        # print(grad_a[k-1].shape)\n",
        "        # print(k)\n",
        "        # print(grad_h[k].shape)\n",
        "        # print(grad_sigmoid(a[k-1]).shape)\n",
        "        grad_a[k-1] =grad_h[k] * grad_sigmoid(a[k-1])\n",
        "    self.grad_b,self.grad_w=grad_b,grad_w\n",
        "\n",
        "  def feed_forward(self,X):\n",
        "    a=[None]*(self.numLayers)\n",
        "    h=[None]*(self.numLayers)\n",
        "    k=0\n",
        "    h[0]=X.T\n",
        "    for k in range(0, self.numLayers-1): #for all layers\n",
        "      a_k=self.b[k]+np.matmul(self.W[k].T,h[k]) #0-based Indexing\n",
        "      h_k=g(a_k)\n",
        "      a[k]=(a_k)\n",
        "      h[k+1]=(h_k)\n",
        "    a[self.numLayers-1]=self.b[self.numLayers-1]+np.matmul(self.W[self.numLayers-1].T,h[self.numLayers-1])\n",
        "    output=o(a[self.numLayers-1])#softmax\n",
        "    self.activation,self.preActivation=h,a\n",
        "    self.y_pred=output\n",
        "\n",
        "  def train(self,trainX,trainY,batch_size,epochs,beta,beta_1,beta_2,neta,t):\n",
        "    self.initialize()\n",
        "\n",
        "    #if optimiser=='nag'\n",
        "    # self.init_nag()\n",
        "\n",
        "    for j in range(0,epochs):\n",
        "      for i in range(0, trainX.shape[0],batch_size):\n",
        "       \n",
        "        #if optimiser=='nag'\n",
        "        # W_copy=self.W\n",
        "        # b_copy=self.b\n",
        "\n",
        "\n",
        "        self.feed_forward(trainX[i:i+batch_size])\n",
        "        self.back_propagation(trainY[i:i+batch_size],batch_size)\n",
        "        Grad_w=np.array(self.grad_w)\n",
        "        for i in range(0,Grad_w.shape[0]):\n",
        "          Grad_w[i]=Grad_w[i].T\n",
        "        Grad_b=np.array(self.grad_b)\n",
        "\n",
        "        # self.update_sgd(neta,Grad_w,Grad_b)\n",
        "        # self.update_mom(neta,beta,Grad_w,Grad_b)\n",
        "        # self.update_nag(neta,beta,Grad_w,Grad_b,W_copy,b_copy)\n",
        "        # self.update_rmsProp(neta,beta,Grad_w,Grad_b)\n",
        "        # self.update_adam(neta,beta_1,beta_2,Grad_w,Grad_b,t)\n",
        "        self.update_nadam(neta,beta_1,beta_2,Grad_w,Grad_b,t)\n",
        "        t+=1\n",
        "        \n",
        "      self.feed_forward(trainX)\n",
        "\n",
        "      print(self.compute_loss(self.y_pred,self.compute_oneHot_y(trainY),trainX.shape[0]))\n",
        "      #loss is computed over the entire Xtrain \n",
        "\n",
        "\n",
        "  def test(self,valX,valY,beta,neta):\n",
        "    self.feed_forward(valX)\n",
        "    print(\"Accuracy: \")\n",
        "    print(self.get_accuracy(valY,self.y_pred.T)*100)\n",
        "\n",
        "  def update_sgd(self,neta,Grad_w,Grad_b):\n",
        "        self.W=self.W-neta*Grad_w\n",
        "        self.b=self.b-neta*Grad_b\n",
        "  \n",
        "  def update_mom(self,neta,beta,Grad_w,Grad_b):\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b\n",
        "        self.W=self.W-neta*self.u_w\n",
        "        self.b=self.b-neta*self.u_b\n",
        "  \n",
        "  def update_nag(self,neta,beta,Grad_w,Grad_b,W_copy,b_copy):\n",
        "        self.W=W_copy\n",
        "        self.b=b_copy\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b\n",
        "        self.W=self.W-neta*self.u_w\n",
        "        self.b=self.b-neta*self.u_b\n",
        "\n",
        "\n",
        "  def init_nag(self):\n",
        "    W_L = []\n",
        "    W_L.append((np.zeros([784,self.numNeurons])))\n",
        "    for i in range (2 , self.numHiddenLayers+1): #Hiddenlayer 1 to last hidden layer (starts from 2 coz first layer is init just above)\n",
        "      W_L.append((np.zeros([self.numNeurons,self.numNeurons])))\n",
        "    W_L.append((np.zeros([self.numNeurons,self.numClasses])))\n",
        "    W_L= np.array(W_L)\n",
        "\n",
        "    b_L = []\n",
        "    for i in range (1 , self.numLayers): #Hiddenlayer1 to last hidden layer\n",
        "      b_L.append(np.zeros((self.numNeurons,1)))\n",
        "    b_L.append(np.zeros((self.numClasses,1)))\n",
        "    b_L= np.array(b_L)\n",
        "    self.W_L=W_L\n",
        "    self.b_L=b_L\n",
        "\n",
        "  def pre_update_nag(self):\n",
        "    self.W_L=self.W-neta*self.u_w\n",
        "    self.b_L=self.b-neta*self.u_b\n",
        "    self.W=self.W_L\n",
        "    self.b=self.b_L\n",
        "\n",
        "  def update_rmsProp(self,neta,beta,Grad_w,Grad_b):\n",
        "        self.u_w=beta*self.u_w+(1-beta)*Grad_w*Grad_w\n",
        "        self.u_b=beta*self.u_b+(1-beta)*Grad_b*Grad_b\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*Grad_w\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*Grad_b\n",
        "  \n",
        "  def update_adam(self,neta,beta_1,beta_2,Grad_w,Grad_b,t):\n",
        "\n",
        "        self.m_w=beta_1*self.m_w+(1-beta_1)*Grad_w\n",
        "        self.m_b=beta_1*self.m_b+(1-beta_1)*Grad_b\n",
        "\n",
        "        self.u_w=beta_2*self.u_w+(1-beta_2)*Grad_w*Grad_w\n",
        "        self.u_b=beta_2*self.u_b+(1-beta_2)*Grad_b*Grad_b\n",
        "\n",
        "        m_w_hat=self.m_w/(1-beta_1**t)\n",
        "        m_b_hat=self.m_b/(1-beta_1**t)\n",
        "\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*m_w_hat\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*m_b_hat\n",
        "\n",
        "  def update_nadam(self,neta,beta_1,beta_2,Grad_w,Grad_b,t):\n",
        "\n",
        "        self.m_w=beta_1*self.m_w+(1-beta_1)*Grad_w\n",
        "        self.m_b=beta_1*self.m_b+(1-beta_1)*Grad_b\n",
        "\n",
        "        self.u_w=beta_2*self.u_w+(1-beta_2)*Grad_w*Grad_w\n",
        "        self.u_b=beta_2*self.u_b+(1-beta_2)*Grad_b*Grad_b\n",
        "\n",
        "        m_w_hat=self.m_w/(1-beta_1**t)\n",
        "        m_b_hat=self.m_b/(1-beta_1**t)\n",
        "\n",
        "        self.W=self.W-(neta/(self.u_w**0.5+10**-8))*(beta_1*m_w_hat+((1-beta_1)*Grad_w)/(1-beta_1**t))\n",
        "        self.b=self.b-(neta/(self.u_b**0.5+10**-8))*(beta_1*m_b_hat+((1-beta_1)*Grad_b)/(1-beta_1**t))\n",
        "\n",
        "  def compute_oneHot_y(self,Y):\n",
        "    oneHot_y=[]\n",
        "    for i in range(0,Y.shape[0]):\n",
        "      temp=np.zeros(self.numClasses)\n",
        "      temp[Y[i]]=1\n",
        "      oneHot_y.append(temp)\n",
        "    oneHot_y=np.array(oneHot_y)\n",
        "    return oneHot_y\n",
        "\n",
        "  def compute_loss(self,y_pred,oneHot_y,numImages):\n",
        "     return (-1.0 * np.sum(np.multiply(oneHot_y.T, np.log(y_pred)))/numImages)\n",
        "\n",
        "\n",
        "  def get_accuracy(self,Y_true,Y_pred):\n",
        "    count = 0\n",
        "    for i in range(Y_true.shape[0]):\n",
        "      max_i = 0\n",
        "      #index of predicted class\n",
        "      max_j = 0\n",
        "      for j in range(10):\n",
        "        if (Y_pred[i][j]>max_i):\n",
        "          max_j = j\n",
        "          max_i = Y_pred[i][j]\n",
        "      if (Y_true[i] == max_j):\n",
        "        count+=1\n",
        "      accuracy=count/Y_true.shape[0]\n",
        "    return accuracy\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rELPAHy-N1A3"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = model()\n",
        "\n",
        "epochs=30\n",
        "neta=0.01\n",
        "beta=0.999\n",
        "beta_1=0.9\n",
        "beta_2=0.999\n",
        "t = 1 # initialize timestep for Adam optimizer\n",
        "\n",
        "\n",
        "Samples=numSamples\n",
        "batch_size=32\n",
        "obj.train(trainX,trainY,batch_size,epochs,beta,beta_1,beta_2,neta,t)\n",
        "#there is no back_prop in test.. so need not actually give beta, beta_1, neta, t etc..!\n",
        "obj.test(valX,valY,beta,neta)"
      ],
      "metadata": {
        "id": "CrisyDM13wVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}